## 梯度消失 ##
假设RNN的公式

$h_t = W_tX_t+W_{ht}h_{t-1}$

$ y_t = softmax(W_o h_t) $

从t时刻到0时刻：

对于$W_x$的导数

$\frac{\partial h_t}{\partial W_t} = \sum_0^t  \frac{\partial h_i}{\partial h_{i-1}} \cdot \cdot \cdot \frac{\partial h_{0}}{\partial W_t}$

$\frac{\partial h_t}{\partial W_t} = \sum_0^t  \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}} \cdot \cdot \cdot \frac{\partial h_{0}}{\partial W_t}$


同样可以求得对$W_{ht}$的导数

_链式求导就像找一条路_

由于RNN的sequence length较大，出现连乘，导致梯度消失或者梯度爆炸

对于LSTM

$h_y = tanh(W_tX_y + W_{ht}h_{ht-1} +b_1)$

__tips:__$tanhx$的导数就是$1-tanh^2(x)$


## LSTM 对梯度消失和梯度爆炸的处理 ##

forget gate: $f_t = \sigma(W_f * [h_{t-1}, x_t] + b_f)$

input gate: $i_t = \sigma(W_i * [h_{t-1}, x_t] + b_i)$

output gate: $o_t = \sigma(W_o * [h_{t-1}, x_t] + b_o)$

隐时状态: $h_t = o_t \dot tanh(c_t)$

$c_t = f_{t} * c_{t-1} + i_t * tanh[W_c * [h_{t-1}, x_t] + b_c$

各个门的激活函数都是sigmoid, 输入接近0或1， 因此

$\frac{\partial c_t}{\partial c_{t-1}} = f_t$

$\frac{\partial h_t}{\partial c_{h-1}} = o_t$

## 灵魂问答 ##

__问题1:链式求导谁对谁求导，求导之后谁传给谁__

$y= Wx+b$链式求导时输入即上一步的导数$\delta x$传入，对x的导数是$W$,

传导过来后是$\delta x \dot W$, 对于W即为$\delta W \dot x$, 对于RNN,

上一步的$h_t$是下一步的输入

__问题2；门是指哪一部分，RNN可以加门么__

门是处理$c_t$和$h_t$

output gate是决定$h_t$的哪些信息留下的

forget gate是决定$c_t$上一步哪些信息留下，input_gate决定新的输入哪些信息加入

$c_t$中的$W_c[h_t, x_t]$是相当于RNN中的input, input经过input_gate进入$c_t$

每一个上一步的信息都与门相乘

只有连乘项的梯度可能爆炸，和时间有关的项才会爆炸

还和$c_t$有关系

__问题3：LSTM中$h_t$和$c_t$链路的区别__

