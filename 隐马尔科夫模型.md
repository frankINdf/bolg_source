---
title: 隐马尔科夫模型
date: 2018-04-26 23:51:53
mathjax: true
tags:
---

使用隐马尔科夫模型可以解决有隐藏状态的观测序列问题，本文是对隐马尔科夫模型的学习记录。

<!-- more -->

隐马尔科夫要解决的问题：
前提：
初始状态矩阵$\pi$
状态转移矩阵A
观测概率矩阵B

假设条件：
齐次假设：$t$时刻的状态只依赖于$t-1$时刻的状态
观测独立性假设：任意时刻状态值依赖此刻的马尔科夫链的状态

在有观测不到的隐藏状态时，解决下面3个问题：
1）给定一个观测序列，怎计算这个序列出现的概率,即概率问题
2）已经有一个观测序列，怎么估计出A、B、$\pi$使其在该序列下出现可能最大，即学习问题
3）在给定观测序列后计算条件概率最大的**状态序列**，预测问题
针对问题1通常使用前向算法和后向算法：
定义
$$
\alpha_t(i) = P(O_1,O_2,...,O_t,i_t=q_i|\lambda)
$$
$A{ij}$代表状态由i转移到j的概率，$b_{j}(o_1)$代表状态j时，观测到$o_1$的概率

对于初始值,在i状态下观测到o_1
$$
\alpha_1 = \pi_ib_i(o1)
$$

对于$t=1,2,...,T-1$,t时刻在j状态，t+1时刻在i状态，在t+1观测到$o_{t+1}$的概率如下：
$$
\alpha_{t+1}(i) = [\sum _{j=1} ^{N} \alpha_{t}(j)a_ji]b_i(o_{t+1})
$$

观测到序列O：
$$
P(O|\lambda) = \sum_{i=1}^{N}\alpha_{T}(i)
$$
后向算法：
$$
\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\lambda)
$$

$t$时刻的状态为$q_i$,从$t+1$到$T$部分观测序列为$o_t+1,o_t+2,...,o_T$的概率为后向概率
最终时刻为T，T+1不存在，所以令$\beta_T(i) = 1$
在$t = T-1,T-2,...,1$可以看到从$T-1$开始递减
$\beta_t(i) = \sum_{j=1}^{N}a_ijb_j(o_{t+1}\beta_{t+1}(j))$
将前面的项代入：
$$
P(O|\lambda) = \sum_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)
$$
可以统一写成
$$
P(O|\lambda) = \sum_{i=1} ^{N}\sum_{j=1} ^{N}\alpha _t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)
$$
单个状态概率的计算公式：
时刻t处于状态$q_i$的概率，
$$
\gamma_t(i) = P(i_t = q_i|O,\lambda)=\frac{i_t=q_i,O|\lambda}{P(O|lambda)}
$$

由于前t个数据观测到qi的概率乘以从后面开始观察到$q_i$的概率
$$
\alpha_ t(i)\beta _t(i) = P(i_t=q_i,O|\lambda)
$$
得到：
$$
\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}
$$
在t时刻处于q_i,t+1时刻处于q_j的概率
$$
\xi_t(i,j) = \frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i,i_{t+1}=q_j,O|\lambda)}
$$

$$
P(i_t=q_i,i_{t+1}=q_j,O|\lambda) = \alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)
$$

$$
\xi_t(i,j) = \frac{\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_t(i)a_{ij}b_{j}(o_{t+1})\beta_{t+1}(j)}
$$

Baum-Welch算法
1.确定对数似然函数$logP(O,I|\lambda)$
2.E步，推断概率分布，并求期望
$$
Q(\lambda,\bar{\lambda}) = E_Ilog(P(O,I|\lambda))
$$

$$
Q(\lambda,\bar{\lambda}) = \sum_{I}logP(O,I|\lambda)P(O,I|\bar{\lambda})
$$

3.M步,计算估计参数下
$$
\pi_i = \frac{P(O,i_1=i|\bar{\lambda})}{P(O|\bar{\lambda})}=\gamma_1(i)
$$

$$
a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\bar{lambda})}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\gamma _t(i)}
$$

$$
b_j(k) = \frac{\sum_{t=1}^{T}P(O,i_t=j)|\bar{\lambda})I(o_t=v_k)}{\sum_{t=1}^{T}P(O,i_t=j|bar{lambda})} = \frac{\sum _{t=1} ^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}
$$


维比特算法
贪心算法，如果当前路径为最优路径，则当前路径前一步也是最优路径。
初始化
$$
\delta_1(i) = pi_ib_i(o_1)
$$

$$
\psi_1(i)=0
$$


递推
$$
\delta_t(i) = max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]b_i(o_1)
$$

$$
\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{t-1}(j)a_{ji}]
$$


终止
$$
\delta_t(i) = max_{1\leqslant j \leqslant N} \delta _T(i)
$$

$$
\psi_t(i) = arg max_{1\leqslant j \leqslant N} [\delta _{T}(i)]
$$

