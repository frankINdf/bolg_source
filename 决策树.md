---
title: 决策树
date: 2018-04-22 23:27:25
mathjax: true
tags:
---



本文主要记录决策树的相关知识和两个例子。

<!-- more -->

决策树划分
**信息增益**
ID3
$$
Gain = Ent(D)-\sum_{v = 1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
$$

$$
Ent (D) = \sum_{k=1}^{|y|}p_klog_{2} p_k
$$
缺点：趋向子节点多的分类
无法处理连续特征
无法处理缺失值
可能过拟合
**信息增益率**
C4.5
$$
Gain_ratio = \frac{Gain(D,a)}{IV(a)}
$$

$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
缺点：趋向子节点少的

**基尼系数**
CART
$$
Gini(D) = \sum_{k=1}^{|y|}a\sum_{k'\neq k}p_kp_{k'}
$$
**CART回归树**
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。

**剪枝**
预剪枝：划分前计算准确率，如果分类后的准确率降低就不进行分类
后剪枝：划分后坍塌，计算准确率，如果准确率提升就不分类

**连续值的处理：**
将连续值划分为N个区间，计算信息增益

**缺失值的处理**
1）对于含有缺失值的数据，分类的属性需要做修正
$$
Gain = e\times Gain( \tilde{D},a)
$$
2）对于含有缺失值的数据，以属性各值所占的概率分配到不同的值当中,概率为$\tilde{\gamma _v}$

**多变量决策树**
非叶节点为分类器$\sum _{d}^{i=1} w_ia_i=t$

**SKLEARN实现决策树**
class sklearn.tree.DecisionTreeClassifier(
criterion=’gini’,  /分类特征选择标准，可以用基尼系数'gini'或者熵'entropy'
splitter=’best’,   /遍历所有特征用'best',随机选取局部最优解用'random','random'适用数据量大时
max_depth=None,    /最大深度
min_samples_split=2,/多少个特征以下停止划分
 min_samples_leaf=1, /最小叶子数，子节点少于多少进行剪枝
min_weight_fraction_leaf=0.0,/针对缺失值，权重小于多少进行剪枝
max_features=None, /最大特征数，'auto','sqrt','log2'或者数字，寻找最佳分割点时的最大特征数。
random_state=None, 
max_leaf_nodes=None,
min_impurity_decrease=0.0,\不纯度减少多少就要进行分类
min_impurity_split=None, \最小不纯度，即纯度大时不再生成子树
class_weight=None, /样本权重,可选'balanced'进行自动计算权重，样本量少的类权重高
presort=False /是否预先排序
)

决策树可视化样例，使用sklearn中的鸢尾花数据：

```
from sklearn.datasets import load_iris
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import subprocess
import pydot
# 使用dot文件进行可视化
# sklearn.tree下面的export_graphviz可以输出dot文件
# 定义决策树，使用默认参数
clf = tree.DecisionTreeClassifier()
iris = load_iris()
# 进行训练
clf = clf.fit(iris.data, iris.target)
# 输出tree.dot
tree.export_graphviz(clf, out_file='tree.dot')
```

生成的决策树如下：

![re](决策树\tree.png)

回归树生成

```
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
rng = np.random.RandomState(1)
# rng.rand(80, 1)生成一个80行1列的随机数，范围为0到1
X = np.sort(5 * rng.rand(80, 1), axis=0)
# 生成y并展开
y = np.sin(X).ravel()
# 以5为步长进行切片，这些位置的数为原来的数字加3*（0.5-随机数）
y[::5] += 3 * (0.5 - rng.rand(16))
# 生成模型判别
# 这里regr_1深度为2，regr_2深度为5
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X, y)
regr_2.fit(X, y)
# 测试数据，从0到5生成500个
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
# 对x进行预测
# 深度为2的树
y_1 = regr_1.predict(X_test)
# 深度为5的树
y_2 = regr_2.predict(X_test)
# Plot the results
plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",
c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",
label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()
```

![策](决策树\决策树.JPG)