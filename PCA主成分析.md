---
title: PCA主成分析
date: 2018-05-18 21:55:07
tags:
---

本文主要是对矩阵运算和PCA主成分析的学习。

#### 矩阵乘法意义

矩阵是线性代数的基础，矩阵相乘运算大家都知道，但这到底代表了什么呢？

关于$AB^T$:矩阵$A$、$B$相乘为A的行向量和每个列向量相乘，第一行$[a1*b1,a1*b2,...]$其实就是$a1$在$b_1,b_2....$这些向量上的投影$|a_i||b_i|cos\theta$。

关于矩阵乘法：矩阵相乘就是把原来每个维度的坐标进行变换$Ax$将原来空间上的坐标旋转，这里说是旋转是因为向量长度不变，只是分量变了.

#### 矩阵特征值和特征向量  

将矩阵的行向量和特征向量相乘$a_ix_i$得到的是在特征向量$x_i$方向的投影,矩阵$Ax_i$得到的就是矩阵中每行在特征向量方向投影的向量，只有矩阵中的向量与特征向量垂直或平行时，才能满足$Ax_i=\lambda x_i$。这样就很好理解矩阵特征值和特征向量以及特征向量之间的关系。

#### PCA主成分析

PCA的思想就是提取特征值较大的特征向量来表示原数据，变换过程可以参考下图

![c](PCA主成分析\pca.jpg)

首先为什么要提取特征值较大的特征向量，因为$Ax = \lambda x$,特征值$\lambda$较大代表A中列向量在x方向的投影较大，即数据分布范围较广，在这个方向投影可以保证数据最多信息，即投影到该方向上后数据的方差最大。

其次怎么将原数据投影呢？根据之前矩阵乘法的意义，其实简单的将数据乘以特征向量就可以得到投影值。

PS:主成分析前需要对数据进行预处理。