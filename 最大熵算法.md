---
title: 最大熵算法
date: 2018-04-23 20:24:20
mathjax: true
tags:
---

最大熵原理和$logistic$、最大熵马科夫模型都有关，本文是最大熵模型的学习记录。

<!-- more -->

模型在满足已有的约束条件的情况下，约束外的部分熵越大，模型越好。
即对于已知部分，要尽可能确定。
对于未知的部分，要保证不确定性。
任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。
举个例子，若只有约束$P_a+P_b+P_c=1$，则最优的模型每次a、b、c出现的概率均相同，因为只有这种情况才没有新增信息。

最大熵模型就是要使熵最大化，同时要满足经验分布的期望和模型的期望相同即$E_p = E _\tilde p$:
$$
{max} H(P) = - \sum _{x,y}\tilde{P}(x)P(x|y)logP(x|y)
$$

$$
s.t.\: \: E_p(f_i) = E_{\tilde{p}}(f_i) , i=1,2,3...
$$

$$
\sum _{y} P(y|x) = 1
$$


其中,$\tilde{p}$为经验分布
约束条件下求极值，可以用拉格朗日乘子法进行求解。

1）写出拉格朗日乘子方程：
$$
L(P,W) = -H(P) + w_0(1-\sum _{y}P(y|x)) + \sum _{i=1}^{n}(E_{\tilde{p}}(f_i) - E_p(f_i))
$$
由$E_\tilde   p = \sum \tilde{P}(x,y)f(x,y)$和$E_p =\sum \tilde{P}(x)P(y|x)f(x,y)$可得：
$$
L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x)+w_0(\sum_{y} P(y|x)-1) \\
+ w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))
$$

L是相当于求$min _{P\in C}max _{w}L(P,w)$

根据对偶原理，可以将$minL(P,w)$转化为求$maxL(P,w)$,问题转化为：
$$\min _{P\in C}\max _{w}L(P,w) = \max _{w} \min _{P\in C}L(P,w)$$
最终需要求解：
$$P_w = arg \min _{P\in C}L(P,w) = L(P,w)$$
求解方法一：
求偏导
$$\frac{\partial L}{\partial P(y|x)} =\sum _{x,y}\tilde{P}(x)(logP(y|x)+1-w_0-\sum ^{n}_{i=1}w_if_i(x,y))$$
偏导为0，可以求出$P(y|x)$:
$$P(y|x) = exp(\sum ^{n} _{i=1}w_if_i(x,y) + w_0-1)$$
由$\sum _{y} P(y|x) = 1$可得
$$exp(1-w_0) = \sum exp(\sum^{n} _{i=1}w_if_i(x,y)) $$
可以得到
$$
P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}
$$
代入L(x)可以求得:
$$
L(P,W) = \sum\tilde{P}(x) P(y|x)logP(y|x) +  w_i(\sum \tilde{P}(x,y)f(x,y)-\sum \tilde{P}(x)P(y|x)f(x,y))
\\=\sum_{x,y}\tilde{P}(x) P(y|x)(logP(y|x) - \sum _{i=1} ^{n}w_if_i(x,y))+\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y) 
\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x,y}\tilde{P}(x) P(y|x)logZ_w(x)
\\=\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)
$$
求解方法二：
最大似然估计求解对数似然函数，参考$logistic$回归中的内容。

条件概率分布$P(X|Y)$的对数似然函数为：
$$
L_\tilde{p}(P_w) = log \prod_{x,y}P(y|x)^{\tilde{p}(x,y)}
$$
将$P(y|x) = \frac{exp(\sum ^{n} _{i=1}w_if_i(x,y))}{Z_w(x)}$代入
$$
L_\tilde{p}(P_w) =\sum_{x,y}\tilde{P}(x) \sum _{i=1} ^{n}w_if_i(x,y)- \sum_{x}\tilde{P}(x) logZ_w(x)
$$
**一点思考**
1.最大熵为什么可以保证模型最优化
2.最大熵算法和LR的联系

可以看到当

3.最大熵模型原理比较重要的推导：

- p(y|x)求和为1
- 经验分布和模型的期望相等
  ​